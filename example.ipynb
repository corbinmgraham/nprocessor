{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASTNode:\n",
    "    pass\n",
    "\n",
    "class NumberNode(ASTNode):\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "        \n",
    "class BinOpNode(ASTNode):\n",
    "    def __init__(self, left, op, right):\n",
    "        self.left = left\n",
    "        self.op = op\n",
    "        self.right = right\n",
    "\n",
    "class UnaryOpNode(ASTNode):\n",
    "    def __init__(self, op, expr):\n",
    "        self.op = op\n",
    "        self.expr = expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parser:\n",
    "    def __init__(self, text):\n",
    "        self.tokens = self.tokenize(text)\n",
    "        self.pos = 0\n",
    "        \n",
    "    def parse(self):\n",
    "        return self.expr()\n",
    "        \n",
    "    def tokenize(self, text):\n",
    "        # Tokenize the input string into a list of tokens\n",
    "        # ...\n",
    "        \n",
    "    def consume(self, expected_token_type):\n",
    "        # Consume the next token if it is of the expected type\n",
    "        # ...\n",
    "        \n",
    "    def expr(self):\n",
    "        # Parse an expression\n",
    "        left = self.term()\n",
    "        \n",
    "        if self.pos < len(self.tokens) and self.tokens[self.pos].type in [\"PLUS\", \"MINUS\"]:\n",
    "            op_token = self.tokens[self.pos]\n",
    "            self.pos += 1\n",
    "            right = self.expr()\n",
    "            return BinOpNode(left, op_token.value, right)\n",
    "        else:\n",
    "            return left\n",
    "        \n",
    "    def term(self):\n",
    "        # Parse a term\n",
    "        left = self.factor()\n",
    "        \n",
    "        if self.pos < len(self.tokens) and self.tokens[self.pos].type in [\"MULTIPLY\", \"DIVIDE\"]:\n",
    "            op_token = self.tokens[self.pos]\n",
    "            self.pos += 1\n",
    "            right = self.term()\n",
    "            return BinOpNode(left, op_token.value, right)\n",
    "        else:\n",
    "            return left\n",
    "        \n",
    "    def factor(self):\n",
    "        # Parse a factor\n",
    "        token = self.tokens[self.pos]\n",
    "        self.pos += 1\n",
    "        \n",
    "        if token.type == \"LPAREN\":\n",
    "            expr_node = self.expr()\n",
    "            self.consume(\"RPAREN\")\n",
    "            return expr_node\n",
    "        elif token.type == \"NUMBER\":\n",
    "            return NumberNode(token.value)\n",
    "        elif token.type == \"PLUS\":\n",
    "            expr_node = self.expr()\n",
    "            return UnaryOpNode(token.value, expr_node)\n",
    "        elif token.type == \"MINUS\":\n",
    "            expr_node = self.expr()\n",
    "            return UnaryOpNode(token.value, expr_node)\n",
    "        else:\n",
    "            raise SyntaxError(\"Unexpected token: {}\".format(token))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '+', '1']\n"
     ]
    }
   ],
   "source": [
    "text = \"2 + 3 * (4 - 1)\"\n",
    "parser = Parser(text)\n",
    "root_node = parser.parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bob': 'Hello World'}\n"
     ]
    }
   ],
   "source": [
    "parser = Parser()\n",
    "parser.parse('Hello World')\n",
    "print(parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('This', 'PRON'), ('is', 'AUX'), ('a', 'DET'), ('sentence', 'NOUN'), ('.', 'PUNCT')]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"This is a sentence.\")\n",
    "print([(w.text, w.pos_) for w in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Corbin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Corbin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK tokens: ['This', 'is', 'a', 'sample', 'sentence', '.']\n",
      "NLTK tags: [('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('sample', 'JJ'), ('sentence', 'NN'), ('.', '.')]\n",
      "SpaCy tokens: ['This', 'is', 'a', 'sample', 'sentence', '.']\n",
      "SpaCy tags: [('This', 'PRON'), ('is', 'AUX'), ('a', 'DET'), ('sample', 'NOUN'), ('sentence', 'NOUN'), ('.', 'PUNCT')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"This is a sample sentence.\"\n",
    "\n",
    "# Tokenize with NLTK\n",
    "nltk_tokens = nltk.word_tokenize(text)\n",
    "\n",
    "# Tag with NLTK\n",
    "nltk_tags = nltk.pos_tag(nltk_tokens)\n",
    "\n",
    "# Tokenize and tag with SpaCy\n",
    "spacy_doc = nlp(text)\n",
    "spacy_tokens = [token.text for token in spacy_doc]\n",
    "spacy_tags = [(token.text, token.pos_) for token in spacy_doc]\n",
    "\n",
    "print(\"NLTK tokens:\", nltk_tokens)\n",
    "print(\"NLTK tags:\", nltk_tags)\n",
    "print(\"SpaCy tokens:\", spacy_tokens)\n",
    "print(\"SpaCy tags:\", spacy_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parser:\n",
    "    world = {}\n",
    "    # nlp = SpacyProcessor\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.world = {}\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return str(self.world)\n",
    "    \n",
    "    def identify(self, doc):\n",
    "        for token in doc:\n",
    "            if ('subj' in token.dep_):\n",
    "                subtree = list(token.subtree)\n",
    "                start = subtree[0].i\n",
    "                end = subtree[-1].i + 1\n",
    "                return doc[start:end]\n",
    "            \n",
    "    def objective(self, doc):\n",
    "        for token in doc:\n",
    "            if('dobj' in token.dep_):\n",
    "                subtree = list(token.subtree)\n",
    "                start = subtree[0].i\n",
    "                end = subtree[-1].i + 1\n",
    "                return doc[start:end]\n",
    "\n",
    "    def parse(self, sentence) -> str:\n",
    "        doc = self.nlp(sentence)\n",
    "        sub = self.identify(doc)\n",
    "        obj = self.objective(doc)\n",
    "        if obj:\n",
    "            print(obj)\n",
    "        self.world[str(sub)] = sentence\n",
    "        return \"Worked.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = [\n",
    "    'The brown fox jumped over the lazy dog.',\n",
    "    'Jane watched TV with her brother when she was tired.',\n",
    "    'Steve Martin is a good actor.',\n",
    "    \"The big black cat stared at the small dog.\",\n",
    "    \"Jane watched her brother in the evenings.\"\n",
    "]\n",
    "\n",
    "sorting_algorithm = [\n",
    "    'There is a line of people at Walmart.',\n",
    "    'The people in this line are Greg, Steve, and Stan.',\n",
    "    'Steve is three feet tall.',\n",
    "    'Greg is two feet tall.',\n",
    "    'Stan is five feet tall.'\n",
    "    # Query: Who is in line? : Steve, Greg, Stan\n",
    "    # Query: What is the order by height? : Stan, Steve, Greg\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'None': 'There is a line of people at Walmart.', 'The people in this line': 'The people in this line are Greg, Steve, and Stan.', 'Steve': 'Steve is three feet tall.', 'Greg': 'Greg is two feet tall.', 'Stan': 'Stan is five feet tall.'}\n"
     ]
    }
   ],
   "source": [
    "parser = Parser()\n",
    "for sentence in sorting_algorithm:\n",
    "    parser.parse(sentence)\n",
    "\n",
    "print(parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subject_phrase(doc):\n",
    "    for token in doc:\n",
    "        if ('subj' in token.dep_):\n",
    "            subtree = list(token.subtree)\n",
    "            start = subtree[0].i\n",
    "            end = subtree[-1].i + 1\n",
    "            return doc[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_object_phrase(doc):\n",
    "    for token in doc:\n",
    "        subtree = list(token.subtree)\n",
    "        start = subtree[0].i\n",
    "        end = subtree[-1].i + 1\n",
    "        return doc[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The brown fox\n",
      "Jane\n"
     ]
    }
   ],
   "source": [
    "for sentence in sents:\n",
    "    doc = nlp(sentence)\n",
    "    sub = get_subject_phrase(doc)\n",
    "    obj = get_object_phrase(doc)\n",
    "    print(sub)\n",
    "    # print(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicate: sat\n",
      "Children: ['cat', 'on', '.']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"The cat sat on the mat.\"\n",
    "\n",
    "doc = nlp(sentence)\n",
    "\n",
    "# Find the root verb of the sentence\n",
    "root = None\n",
    "for token in doc:\n",
    "    if token.dep_ == \"ROOT\" and token.pos_ == \"VERB\":\n",
    "        root = token\n",
    "        break\n",
    "\n",
    "if root:\n",
    "    # Print the root verb and its children\n",
    "    print(\"Predicate:\", root.text)\n",
    "    print(\"Children:\", [child.text for child in root.children])\n",
    "else:\n",
    "    print(\"No predicate found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Corbin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Corbin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Error loading chunking: Package 'chunking' not found in\n",
      "[nltk_data]     index\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download the required NLTK resources (only need to do this once)\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.download(\"chunking\")\n",
    "\n",
    "sentence = \"What is 2 plus 2?\"\n",
    "\n",
    "# Tokenize the sentence into words\n",
    "words = nltk.word_tokenize(sentence)\n",
    "\n",
    "# Perform part-of-speech tagging on the words\n",
    "tagged_words = nltk.pos_tag(words)\n",
    "\n",
    "# Define a chunk grammar to extract the math expression\n",
    "chunk_grammar = r\"\"\"\n",
    "    NP: {<CD>+}            # One or more cardinal numbers\n",
    "        {<NN.*>+}         # One or more nouns\n",
    "        {<SYM>}           # A symbol representing the operator\n",
    "        {<CD>+}          # One or more cardinal numbers\n",
    "\"\"\"\n",
    "\n",
    "# Create a chunk parser using the chunk grammar\n",
    "chunk_parser = nltk.RegexpParser(chunk_grammar)\n",
    "\n",
    "# Parse the tagged words using the chunk parser\n",
    "parsed_sentence = chunk_parser.parse(tagged_words)\n",
    "\n",
    "# Find the math expression in the parse tree\n",
    "math_expr = None\n",
    "for subtree in parsed_sentence.subtrees():\n",
    "    if subtree.label() == \"NP\":\n",
    "        # Extract the words from the subtree and join them into a string\n",
    "        math_expr = \" \".join([word for word, pos in subtree.leaves()])\n",
    "        break\n",
    "\n",
    "if math_expr:\n",
    "    # Perform the calculation\n",
    "    result = eval(math_expr)\n",
    "    print(\"Result:\", result)\n",
    "else:\n",
    "    print(\"No math expression found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('quick', 'JJ'),\n",
       " ('brown', 'NN'),\n",
       " ('fox', 'NN'),\n",
       " ('jumped', 'VBD'),\n",
       " ('over', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('lazy', 'JJ'),\n",
       " ('dog', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"The quick brown fox jumped over the lazy dog.\"\n",
    "words = nltk.tokenize.word_tokenize(sentence)\n",
    "pos = nltk.pos_tag(words)\n",
    "pos"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
